#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”§ IEEE Fraud Detection - Feature Engineering Pipeline
Created for IEEE-CIS Fraud Detection Competition

ç‰¹å¾å·¥ç¨‹ç­–ç•¥ (Feature Engineering Strategy):
1. æ—¶é—´ç‰¹å¾ - Temporal features from transaction timestamps
2. èšåˆç‰¹å¾ - Aggregation features from user behavior patterns
3. è®¾å¤‡æŒ‡çº¹ - Device fingerprinting from identity data
4. é£é™©è¯„åˆ† - Risk scoring features and anomaly indicators
5. äº¤äº’ç‰¹å¾ - Interaction features between transaction and identity
"""

import pandas as pd
import numpy as np
import warnings
from pathlib import Path
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import joblib
import os

warnings.filterwarnings('ignore')

class FraudFeatureEngineer:
    """æ¬ºè¯ˆæ£€æµ‹ç‰¹å¾å·¥ç¨‹ç±»"""
    
    def __init__(self, save_transformers=True):
        self.save_transformers = save_transformers
        self.label_encoders = {}
        self.scaler = StandardScaler()
        self.feature_names = []
        
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("ğŸ”„ Loading fraud detection dataset...")
        
        data_path = Path("ieee-fraud-detection")
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        train_transaction = pd.read_csv(data_path / "train_transaction.csv")
        train_identity = pd.read_csv(data_path / "train_identity.csv")
        
        print(f"âœ… Transaction data: {train_transaction.shape}")
        print(f"âœ… Identity data: {train_identity.shape}")
        
        return train_transaction, train_identity
    
    def create_temporal_features(self, df):
        """åˆ›å»ºæ—¶é—´ç‰¹å¾"""
        print("â° Creating temporal features...")
        
        # TransactionDTæ˜¯ä»å‚è€ƒæ—¶é—´å¼€å§‹çš„ç§’æ•°
        df['TransactionDT_hour'] = (df['TransactionDT'] / 3600) % 24
        df['TransactionDT_day'] = (df['TransactionDT'] / (3600 * 24)) % 7
        df['TransactionDT_week'] = (df['TransactionDT'] / (3600 * 24 * 7))
        
        # æ—¶é—´ç›¸å…³çš„å‘¨æœŸæ€§ç‰¹å¾
        df['hour_sin'] = np.sin(2 * np.pi * df['TransactionDT_hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['TransactionDT_hour'] / 24)
        df['day_sin'] = np.sin(2 * np.pi * df['TransactionDT_day'] / 7)
        df['day_cos'] = np.cos(2 * np.pi * df['TransactionDT_day'] / 7)
        
        # æ˜¯å¦ä¸ºå·¥ä½œæ—¶é—´/å‘¨æœ«ç­‰
        df['is_weekend'] = (df['TransactionDT_day'] >= 5).astype(int)
        df['is_night'] = ((df['TransactionDT_hour'] >= 22) | (df['TransactionDT_hour'] <= 6)).astype(int)
        df['is_business_hours'] = ((df['TransactionDT_hour'] >= 9) & (df['TransactionDT_hour'] <= 17)).astype(int)
        
        temporal_features = [
            'TransactionDT_hour', 'TransactionDT_day', 'TransactionDT_week',
            'hour_sin', 'hour_cos', 'day_sin', 'day_cos',
            'is_weekend', 'is_night', 'is_business_hours'
        ]
        
        print(f"âœ… Created {len(temporal_features)} temporal features")
        return df, temporal_features
    
    def create_transaction_features(self, df):
        """åˆ›å»ºäº¤æ˜“ç›¸å…³ç‰¹å¾"""
        print("ğŸ’° Creating transaction features...")
        
        # é‡‘é¢ç›¸å…³ç‰¹å¾
        df['TransactionAmt_log'] = np.log1p(df['TransactionAmt'])
        df['TransactionAmt_decimal'] = ((df['TransactionAmt'] - df['TransactionAmt'].astype(int)) * 1000).astype(int)
        df['TransactionAmt_isRound'] = (df['TransactionAmt'] == df['TransactionAmt'].round()).astype(int)
        
        # é‡‘é¢åˆ†ç»„ç‰¹å¾
        df['amt_range'] = pd.cut(df['TransactionAmt'], 
                                bins=[0, 50, 100, 300, 1000, float('inf')], 
                                labels=[0, 1, 2, 3, 4]).astype(int)
        
        transaction_features = [
            'TransactionAmt_log', 'TransactionAmt_decimal', 
            'TransactionAmt_isRound', 'amt_range'
        ]
        
        print(f"âœ… Created {len(transaction_features)} transaction features")
        return df, transaction_features
    
    def create_aggregation_features(self, df):
        """åˆ›å»ºèšåˆç‰¹å¾"""
        print("ğŸ“Š Creating aggregation features...")
        
        aggregation_features = []
        
        # æŒ‰å¡ç‰‡èšåˆ
        if 'card1' in df.columns:
            card_agg = df.groupby('card1')['TransactionAmt'].agg(['count', 'mean', 'std', 'min', 'max'])
            card_agg.columns = [f'card1_{col}' for col in card_agg.columns]
            df = df.merge(card_agg, left_on='card1', right_index=True, how='left')
            aggregation_features.extend(card_agg.columns.tolist())
        
        # æŒ‰é‚®ç®±åŸŸåèšåˆ
        if 'P_emaildomain' in df.columns:
            email_agg = df.groupby('P_emaildomain')['TransactionAmt'].agg(['count', 'mean'])
            email_agg.columns = [f'email_{col}' for col in email_agg.columns]
            df = df.merge(email_agg, left_on='P_emaildomain', right_index=True, how='left')
            aggregation_features.extend(email_agg.columns.tolist())
        
        # æŒ‰æ”¶ä»¶äººé‚®ç®±åŸŸåèšåˆ
        if 'R_emaildomain' in df.columns:
            r_email_agg = df.groupby('R_emaildomain')['TransactionAmt'].agg(['count', 'mean'])
            r_email_agg.columns = [f'r_email_{col}' for col in r_email_agg.columns]
            df = df.merge(r_email_agg, left_on='R_emaildomain', right_index=True, how='left')
            aggregation_features.extend(r_email_agg.columns.tolist())
        
        print(f"âœ… Created {len(aggregation_features)} aggregation features")
        return df, aggregation_features
    
    def create_device_features(self, train_transaction, train_identity):
        """åˆ›å»ºè®¾å¤‡æŒ‡çº¹ç‰¹å¾"""
        print("ğŸ“± Creating device fingerprinting features...")
        
        device_features = []
        
        # åˆå¹¶èº«ä»½æ•°æ®
        df = train_transaction.merge(train_identity, on='TransactionID', how='left')
        
        # è®¾å¤‡ä¿¡æ¯ç‰¹å¾
        device_cols = ['DeviceType', 'DeviceInfo']
        for col in device_cols:
            if col in df.columns:
                # è®¾å¤‡ç±»å‹ç¼–ç 
                if df[col].dtype == 'object':
                    le = LabelEncoder()
                    df[f'{col}_encoded'] = le.fit_transform(df[col].fillna('unknown'))
                    self.label_encoders[col] = le
                    device_features.append(f'{col}_encoded')
        
        # æµè§ˆå™¨å’Œæ“ä½œç³»ç»Ÿç‰¹å¾
        browser_cols = [col for col in df.columns if 'id_' in col and col.endswith('browser')]
        os_cols = [col for col in df.columns if 'id_' in col and col.endswith('os')]
        
        for col in browser_cols + os_cols:
            if col in df.columns and df[col].dtype == 'object':
                le = LabelEncoder()
                df[f'{col}_encoded'] = le.fit_transform(df[col].fillna('unknown'))
                self.label_encoders[col] = le
                device_features.append(f'{col}_encoded')
        
        # å±å¹•åˆ†è¾¨ç‡ç‰¹å¾
        if 'id_33' in df.columns:  # å±å¹•åˆ†è¾¨ç‡
            df['screen_width'] = df['id_33'].str.extract('(\d+)x').astype(float)
            df['screen_height'] = df['id_33'].str.extract('x(\d+)').astype(float)
            df['screen_ratio'] = df['screen_width'] / (df['screen_height'] + 1e-5)
            device_features.extend(['screen_width', 'screen_height', 'screen_ratio'])
        
        print(f"âœ… Created {len(device_features)} device features")
        return df, device_features
    
    def create_risk_features(self, df):
        """åˆ›å»ºé£é™©è¯„åˆ†ç‰¹å¾"""
        print("âš ï¸ Creating risk scoring features...")
        
        risk_features = []
        
        # é‡‘é¢å¼‚å¸¸æ£€æµ‹
        df['amt_zscore'] = (df['TransactionAmt'] - df['TransactionAmt'].mean()) / df['TransactionAmt'].std()
        df['amt_is_outlier'] = (np.abs(df['amt_zscore']) > 3).astype(int)
        risk_features.extend(['amt_zscore', 'amt_is_outlier'])
        
        # äº¤æ˜“é¢‘ç‡å¼‚å¸¸ï¼ˆå¦‚æœæœ‰ç”¨æˆ·æ ‡è¯†ï¼‰
        if 'card1' in df.columns:
            card_freq = df['card1'].value_counts()
            df['card_frequency'] = df['card1'].map(card_freq)
            df['is_high_freq_card'] = (df['card_frequency'] > df['card_frequency'].quantile(0.95)).astype(int)
            risk_features.extend(['card_frequency', 'is_high_freq_card'])
        
        # æ—¶é—´å¼‚å¸¸ï¼ˆæ·±å¤œäº¤æ˜“ç­‰ï¼‰
        df['night_transaction_risk'] = (df['is_night'] & (df['TransactionAmt'] > df['TransactionAmt'].median())).astype(int)
        df['weekend_large_transaction'] = (df['is_weekend'] & (df['TransactionAmt'] > df['TransactionAmt'].quantile(0.9))).astype(int)
        risk_features.extend(['night_transaction_risk', 'weekend_large_transaction'])
        
        print(f"âœ… Created {len(risk_features)} risk features")
        return df, risk_features
    
    def handle_missing_values(self, df):
        """å¤„ç†ç¼ºå¤±å€¼"""
        print("ğŸ”§ Handling missing values...")
        
        # æ•°å€¼å‹ç‰¹å¾ï¼šç”¨ä¸­ä½æ•°å¡«å……
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col != 'isFraud' and df[col].isnull().sum() > 0:
                median_val = df[col].median()
                df[col] = df[col].fillna(median_val)
        
        # åˆ†ç±»ç‰¹å¾ï¼šç”¨'unknown'å¡«å……
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            df[col] = df[col].fillna('unknown')
        
        print("âœ… Missing values handled")
        return df
    
    def encode_categorical_features(self, df):
        """ç¼–ç åˆ†ç±»ç‰¹å¾"""
        print("ğŸ·ï¸ Encoding categorical features...")
        
        categorical_cols = df.select_dtypes(include=['object']).columns
        categorical_cols = [col for col in categorical_cols if col not in ['TransactionID']]
        
        encoded_features = []
        for col in categorical_cols:
            if col not in self.label_encoders:
                le = LabelEncoder()
                df[f'{col}_encoded'] = le.fit_transform(df[col])
                self.label_encoders[col] = le
                encoded_features.append(f'{col}_encoded')
            else:
                # ä½¿ç”¨å·²ä¿å­˜çš„ç¼–ç å™¨
                df[f'{col}_encoded'] = self.label_encoders[col].transform(df[col])
                encoded_features.append(f'{col}_encoded')
        
        print(f"âœ… Encoded {len(encoded_features)} categorical features")
        return df, encoded_features
    
    def create_interaction_features(self, df):
        """åˆ›å»ºäº¤äº’ç‰¹å¾"""
        print("ğŸ”„ Creating interaction features...")
        
        interaction_features = []
        
        # é‡‘é¢ä¸æ—¶é—´çš„äº¤äº’
        if 'TransactionAmt_log' in df.columns and 'TransactionDT_hour' in df.columns:
            df['amt_hour_interaction'] = df['TransactionAmt_log'] * df['TransactionDT_hour']
            interaction_features.append('amt_hour_interaction')
        
        # å¡ç‰‡ä¸é‡‘é¢çš„äº¤äº’
        if 'card1_mean' in df.columns:
            df['amt_vs_card_avg'] = df['TransactionAmt'] / (df['card1_mean'] + 1e-5)
            interaction_features.append('amt_vs_card_avg')
        
        # è®¾å¤‡ä¸æ—¶é—´çš„äº¤äº’
        if 'DeviceType_encoded' in df.columns and 'is_night' in df.columns:
            df['device_night_interaction'] = df['DeviceType_encoded'] * df['is_night']
            interaction_features.append('device_night_interaction')
        
        print(f"âœ… Created {len(interaction_features)} interaction features")
        return df, interaction_features
    
    def engineer_features(self, train_transaction, train_identity):
        """ä¸»è¦çš„ç‰¹å¾å·¥ç¨‹å‡½æ•°"""
        print("\n" + "="*60)
        print("ğŸ”§ STARTING FEATURE ENGINEERING PIPELINE")
        print("="*60)
        
        all_features = []
        
        # 1. åˆ›å»ºæ—¶é—´ç‰¹å¾
        train_transaction, temporal_features = self.create_temporal_features(train_transaction)
        all_features.extend(temporal_features)
        
        # 2. åˆ›å»ºäº¤æ˜“ç‰¹å¾
        train_transaction, transaction_features = self.create_transaction_features(train_transaction)
        all_features.extend(transaction_features)
        
        # 3. åˆ›å»ºèšåˆç‰¹å¾
        train_transaction, aggregation_features = self.create_aggregation_features(train_transaction)
        all_features.extend(aggregation_features)
        
        # 4. åˆ›å»ºè®¾å¤‡ç‰¹å¾
        df_with_identity, device_features = self.create_device_features(train_transaction, train_identity)
        all_features.extend(device_features)
        
        # 5. åˆ›å»ºé£é™©ç‰¹å¾
        df_with_identity, risk_features = self.create_risk_features(df_with_identity)
        all_features.extend(risk_features)
        
        # 6. å¤„ç†ç¼ºå¤±å€¼
        df_with_identity = self.handle_missing_values(df_with_identity)
        
        # 7. ç¼–ç åˆ†ç±»ç‰¹å¾
        df_with_identity, encoded_features = self.encode_categorical_features(df_with_identity)
        all_features.extend(encoded_features)
        
        # 8. åˆ›å»ºäº¤äº’ç‰¹å¾
        df_with_identity, interaction_features = self.create_interaction_features(df_with_identity)
        all_features.extend(interaction_features)
        
        # ä¿å­˜ç‰¹å¾åç§°
        self.feature_names = all_features
        
        print(f"\nğŸ“Š FEATURE ENGINEERING SUMMARY:")
        print(f"   â€¢ Original transaction features: {len(train_transaction.columns)}")
        print(f"   â€¢ Original identity features: {len(train_identity.columns)}")
        print(f"   â€¢ Engineered features: {len(all_features)}")
        print(f"   â€¢ Total features in dataset: {len(df_with_identity.columns)}")
        
        # ä¿å­˜è½¬æ¢å™¨
        if self.save_transformers:
            self.save_feature_transformers()
        
        return df_with_identity, all_features
    
    def save_feature_transformers(self):
        """ä¿å­˜ç‰¹å¾è½¬æ¢å™¨"""
        os.makedirs('models', exist_ok=True)
        
        # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
        joblib.dump(self.label_encoders, 'models/label_encoders.pkl')
        
        # ä¿å­˜ç‰¹å¾åç§°
        joblib.dump(self.feature_names, 'models/feature_names.pkl')
        
        print("âœ… Feature transformers saved to models/")
    
    def prepare_model_data(self, df, target_col='isFraud', test_size=0.2, random_state=42):
        """å‡†å¤‡æ¨¡å‹è®­ç»ƒæ•°æ®"""
        print("\nğŸ“‹ Preparing data for modeling...")
        
        # é€‰æ‹©ç‰¹å¾åˆ—
        feature_cols = [col for col in self.feature_names if col in df.columns]
        
        # ç¡®ä¿æ²¡æœ‰æ— é™å€¼æˆ–NaN
        for col in feature_cols:
            if col in df.columns:
                df[col] = df[col].replace([np.inf, -np.inf], np.nan)
        df[feature_cols] = df[feature_cols].fillna(0)
        
        X = df[feature_cols]
        y = df[target_col]
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # ä¿å­˜ç¼©æ”¾å™¨
        if self.save_transformers:
            joblib.dump(self.scaler, 'models/feature_scaler.pkl')
        
        print(f"âœ… Data prepared:")
        print(f"   â€¢ Training set: {X_train.shape}")
        print(f"   â€¢ Test set: {X_test.shape}")
        print(f"   â€¢ Features used: {len(feature_cols)}")
        print(f"   â€¢ Fraud rate in train: {y_train.mean():.4f}")
        print(f"   â€¢ Fraud rate in test: {y_test.mean():.4f}")
        
        return X_train_scaled, X_test_scaled, y_train, y_test, feature_cols

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ IEEE Fraud Detection - Feature Engineering Starting...")
    print("="*70)
    
    # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
    fe = FraudFeatureEngineer(save_transformers=True)
    
    # åŠ è½½æ•°æ®
    train_transaction, train_identity = fe.load_data()
    
    # æ‰§è¡Œç‰¹å¾å·¥ç¨‹
    engineered_df, feature_list = fe.engineer_features(train_transaction, train_identity)
    
    # å‡†å¤‡æ¨¡å‹æ•°æ®
    X_train, X_test, y_train, y_test, feature_cols = fe.prepare_model_data(engineered_df)
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    os.makedirs('data', exist_ok=True)
    
    # ä¿å­˜è®­ç»ƒæ•°æ®
    train_data = {
        'X_train': X_train,
        'X_test': X_test, 
        'y_train': y_train,
        'y_test': y_test,
        'feature_names': feature_cols
    }
    
    joblib.dump(train_data, 'data/processed_train_data.pkl')
    
    # ä¿å­˜å®Œæ•´çš„å·¥ç¨‹åŒ–æ•°æ®é›†
    engineered_df.to_csv('data/engineered_features.csv', index=False)
    
    print(f"\nğŸ‰ Feature engineering completed successfully!")
    print(f"âœ… Processed data saved to data/")
    print(f"âœ… Model transformers saved to models/")
    print(f"ğŸ¯ Ready for baseline model training!")

if __name__ == "__main__":
    main() 